{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API keys from .env file\n",
    "load_dotenv()\n",
    "OPENWEATHER_API_KEY = os.getenv(\"OPENWEATHER_API_KEY\")\n",
    "GOOGLE_MAPS_API_KEY = os.getenv(\"GOOGLE_MAPS_API_KEY\")\n",
    "# Ensure API keys are loaded, otherwise raise a helpful error\n",
    "if not OPENWEATHER_API_KEY or not GOOGLE_MAPS_API_KEY:\n",
    "    print(\"Warning: API keys not found in .env file. Create a .env file with OPENWEATHER_API_KEY and GOOGLE_MAPS_API_KEY.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import random\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('travel_recommender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path='holidify.csv'):\n",
    "    \"\"\"\n",
    "    Load and preprocess the tourism dataset\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the CSV file containing tourism data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Preprocessed tourism dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Loading data from {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Remove duplicates based on City column\n",
    "        initial_count = len(df)\n",
    "        df = df.drop_duplicates(subset=['City'])\n",
    "        logger.info(f\"Removed {initial_count - len(df)} duplicate cities\")\n",
    "        \n",
    "        # Clean city names (remove extra spaces)\n",
    "        df['City'] = df['City'].str.strip()\n",
    "        \n",
    "        # Fill NaN values in 'Best Time to visit'\n",
    "        df['Best Time to visit'] = df['Best Time to visit'].fillna('Throughout the year')\n",
    "        \n",
    "        # Create a column indicating if a destination is good to visit year-round\n",
    "        df['Year_round'] = df['Best Time to visit'].str.contains('Throughout the year').astype(int)\n",
    "        \n",
    "        # Add columns for weather and traffic (will be filled later)\n",
    "        df['Weather Quality'] = np.nan\n",
    "        df['Traffic Level'] = np.nan\n",
    "        \n",
    "        logger.info(f\"Successfully loaded and preprocessed {len(df)} destinations\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_recommendation_system(current_location, travel_date, avoid_crowds=True, \n",
    "                                   weather_preference=\"clear\", budget_level=\"medium\", \n",
    "                                   use_real_apis=True):\n",
    "    \"\"\"\n",
    "    Configure the recommendation system with user preferences\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    current_location : str\n",
    "        User's current city\n",
    "    travel_date : str or datetime\n",
    "        Planned travel date (YYYY-MM-DD if string)\n",
    "    avoid_crowds : bool\n",
    "        Whether to prioritize destinations with less traffic\n",
    "    weather_preference : str\n",
    "        Preferred weather condition (\"clear\", \"moderate\", etc.)\n",
    "    budget_level : str\n",
    "        Budget preference (\"low\", \"medium\", \"high\")\n",
    "    use_real_apis : bool\n",
    "        Whether to use real API calls or mock data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Configuration dictionary with all parameters\n",
    "    \"\"\"\n",
    "    # Convert string date to datetime\n",
    "    if isinstance(travel_date, str):\n",
    "        travel_date = datetime.datetime.strptime(travel_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    # Map text preferences to numerical values for the model\n",
    "    weather_preference_map = {\n",
    "        \"clear\": 9,\n",
    "        \"moderate\": 7,\n",
    "        \"cool\": 5,\n",
    "        \"warm\": 8,\n",
    "        \"rainy\": 3,\n",
    "        \"snowy\": 4\n",
    "    }\n",
    "    \n",
    "    budget_level_map = {\n",
    "        \"low\": 1,\n",
    "        \"medium\": 2,\n",
    "        \"high\": 3\n",
    "    }\n",
    "    \n",
    "    # Create configuration dictionary\n",
    "    config = {\n",
    "        \"current_location\": current_location,\n",
    "        \"travel_date\": travel_date,\n",
    "        \"avoid_crowds\": avoid_crowds,\n",
    "        \"weather_preference_text\": weather_preference,\n",
    "        \"weather_preference_score\": weather_preference_map.get(weather_preference.lower(), 7),\n",
    "        \"budget_level_text\": budget_level,\n",
    "        \"budget_level_score\": budget_level_map.get(budget_level.lower(), 2),\n",
    "        \"use_real_apis\": use_real_apis,\n",
    "        \"api_keys\": {\n",
    "            \"weather\": OPENWEATHER_API_KEY,\n",
    "            \"maps\": GOOGLE_MAPS_API_KEY\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Configured system for travel from {current_location} on {travel_date.strftime('%Y-%m-%d')}\")\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weather Data Retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import logging\n",
    "import random\n",
    "\n",
    "# Logger setup\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_weather_data(cities, api_key, use_real_api=True):\n",
    "    api_key = \"6de43994eab17f60d55c448f8162c3f7\"\n",
    "    \"\"\"\n",
    "    Get current weather data for a list of cities\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cities : list\n",
    "        List of city names\n",
    "    api_key : str\n",
    "        OpenWeatherMap API key\n",
    "    use_real_api : bool\n",
    "        Whether to use real API or dummy data for testing\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with weather data for each city\n",
    "    \"\"\"\n",
    "    weather_data = {}\n",
    "\n",
    "    if not use_real_api:\n",
    "        # Generate mock data for testing\n",
    "        logger.info(\"Using mock weather data\")\n",
    "        weather_conditions = [\"clear sky\", \"few clouds\", \"scattered clouds\", \"moderate rain\", \"light rain\", \"sunny\"]\n",
    "        for city in cities:\n",
    "            temp = round(random.uniform(15, 35), 2)  # Random temp between 15-35Â°C\n",
    "            condition = random.choice(weather_conditions)\n",
    "            quality = 10 if 20 <= temp <= 30 and \"rain\" not in condition else (7 if 15 <= temp <= 35 else 4)\n",
    "            \n",
    "            weather_data[city] = {\n",
    "                \"condition\": condition,\n",
    "                \"temp\": temp,\n",
    "                \"quality\": quality\n",
    "            }\n",
    "        return weather_data\n",
    "\n",
    "    # Use real API\n",
    "    logger.info(f\"Fetching weather data for {len(cities)} cities\")\n",
    "\n",
    "    for city in cities:\n",
    "        try:\n",
    "            # OpenWeatherMap API URL (Corrected)\n",
    "            url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric\"\n",
    "            response = requests.get(url)\n",
    "            data = response.json()\n",
    "\n",
    "            if response.status_code == 200 and \"main\" in data and \"weather\" in data:\n",
    "                temp = data[\"main\"][\"temp\"]\n",
    "                condition = data[\"weather\"][0][\"description\"]\n",
    "\n",
    "                # Calculate weather quality score (higher is better)\n",
    "                if \"clear\" in condition or \"sunny\" in condition:\n",
    "                    quality = 9\n",
    "                elif \"cloud\" in condition:\n",
    "                    quality = 7\n",
    "                elif \"rain\" in condition:\n",
    "                    quality = 4\n",
    "                elif \"snow\" in condition:\n",
    "                    quality = 3\n",
    "                else:\n",
    "                    quality = 6\n",
    "\n",
    "                # Adjust for temperature\n",
    "                if 20 <= temp <= 30:\n",
    "                    quality += 1\n",
    "                elif temp < 5 or temp > 40:\n",
    "                    quality -= 2\n",
    "\n",
    "                # Ensure score is between 1-10\n",
    "                quality = max(1, min(10, quality))\n",
    "\n",
    "                weather_data[city] = {\n",
    "                    \"condition\": condition,\n",
    "                    \"temp\": temp,\n",
    "                    \"quality\": quality\n",
    "                }\n",
    "            else:\n",
    "                logger.warning(f\"Failed to get weather for {city}: {data.get('message', 'Unknown error')}\")\n",
    "                weather_data[city] = {\"condition\": \"unknown\", \"temp\": None, \"quality\": 5}\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching weather for {city}: {str(e)}\")\n",
    "            weather_data[city] = {\"condition\": \"error\", \"temp\": None, \"quality\": 5}\n",
    "\n",
    "    return weather_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traffic Data Retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import logging\n",
    "import random\n",
    "\n",
    "# Logger setup\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_traffic_data(cities, current_location, api_key, use_real_api=True):\n",
    "    api_key = \"tjAnW2bFSmzSJJMx7NH1Y5MRunBIaxWCJqyWKFkm4deivY3pb7hG3QdKQvYrAoC5\"\n",
    "    \"\"\"\n",
    "    Get traffic data for a list of cities and their popular places\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cities : list\n",
    "        List of city names\n",
    "    current_location : str\n",
    "        User's current location\n",
    "    api_key : str\n",
    "        Google Maps API key\n",
    "    use_real_api : bool\n",
    "        Whether to use real API or dummy data for testing\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with traffic data for each city\n",
    "    \"\"\"\n",
    "    traffic_data = {}\n",
    "    \n",
    "    if not use_real_api:\n",
    "        # Generate mock data for testing\n",
    "        logger.info(\"Using mock traffic data\")\n",
    "        for city in cities:\n",
    "            traffic_level = random.randint(2, 9)# Mock overall traffic level (1-10 scale)\n",
    "            places = {f\"{city} Place {i}\": random.randint(15, 60) for i in range(1, 6)}  # Mock travel time\n",
    "                \n",
    "            traffic_data[city] = {\n",
    "                \"traffic_level\": traffic_level,\n",
    "                \"places\": places\n",
    "            }\n",
    "        return traffic_data\n",
    "    \n",
    "    #Use real Distance Matrix API\n",
    "    logger.info(f\"Fetching traffic data for {len(cities)} cities from {current_location}\")\n",
    "    try:\n",
    "        # Construct destinations query string (latitude,longitude format)\n",
    "        destinations = \"|\".join(cities)\n",
    "\n",
    "        # Corrected Google Distance Matrix API request\n",
    "        url = (\n",
    "            f\"https://api.distancematrix.ai/maps/api/distancematrix/json?origins=51.4822656,-0.1933769&destinations=51.4994794,-0.1269979&key=tjAnW2bFSmzSJJMx7NH1Y5MRunBIaxWCJqyWKFkm4deivY3pb7hG3QdKQvYrAoC5\"\n",
    "            f\"origins={current_location}&destinations={destinations}&key={api_key}&departure_time=now&traffic_model=best_guess\"\n",
    "        )\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "\n",
    "        if data[\"status\"] != \"OK\":\n",
    "            logger.error(f\"Error in API response: {data}\")\n",
    "            return traffic_data\n",
    "        \n",
    "        for i, city in enumerate(cities):\n",
    "            try:\n",
    "                element = data[\"rows\"][0][\"elements\"][i]\n",
    "                if element[\"status\"] == \"OK\":\n",
    "                    travel_time = element[\"duration_in_traffic\"][\"value\"] // 60  # Convert to minutes\n",
    "                    traffic_level = min(max(travel_time // 10, 1), 10)  # Normalize traffic level (1-10 scale)\n",
    "                else:\n",
    "                    travel_time = random.randint(20, 60)  # Fallback value\n",
    "                    traffic_level = random.randint(3, 7)\n",
    "\n",
    "                places = {f\"{city} Attraction {i}\": travel_time for i in range(1, 6)}\n",
    "\n",
    "                traffic_data[city] = {\n",
    "                    \"traffic_level\": traffic_level,\n",
    "                    \"places\": places\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing traffic data for {city}: {str(e)}\")\n",
    "                traffic_data[city] = {\"traffic_level\": 5, \"places\": {}}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to fetch traffic data: {str(e)}\")\n",
    "\n",
    "    return traffic_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Season check Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_season(city_info, current_date):\n",
    "    \"\"\"\n",
    "    Determine if a destination is currently in its recommended visit season\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    city_info : dict or Series\n",
    "        Information about the city including 'Best Time to visit'\n",
    "    current_date : datetime\n",
    "        The date to check against\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if the city is in season, False otherwise\n",
    "    \"\"\"\n",
    "    best_time = city_info['Best Time to visit']\n",
    "    current_month = current_date.strftime('%B')\n",
    "    \n",
    "    # If it's good year-round\n",
    "    if 'Throughout the year' in best_time:\n",
    "        return True\n",
    "    \n",
    "    # Parse the best time information\n",
    "    month_names = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \n",
    "                   \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "    \n",
    "    # Handle different formats of best time information\n",
    "    if 'to' in best_time:\n",
    "        # Format like \"October to March\"\n",
    "        parts = best_time.split('to')\n",
    "        start_month = parts[0].strip()\n",
    "        end_month = parts[1].strip()\n",
    "        \n",
    "        # Extract month names from text\n",
    "        start_month_match = next((m for m in month_names if m.lower() in start_month.lower()), None)\n",
    "        end_month_match = next((m for m in month_names if m.lower() in end_month.lower()), None)\n",
    "        \n",
    "        if start_month_match and end_month_match:\n",
    "            start_idx = month_names.index(start_month_match)\n",
    "            end_idx = month_names.index(end_month_match)\n",
    "            \n",
    "            # Handle ranges that span across year-end\n",
    "            if start_idx > end_idx:\n",
    "                months = month_names[start_idx:] + month_names[:end_idx+1]\n",
    "            else:\n",
    "                months = month_names[start_idx:end_idx+1]\n",
    "                \n",
    "            return current_month in months\n",
    "    else:\n",
    "        # Format might have multiple periods or just be a list of months\n",
    "        for month in month_names:\n",
    "            if month.lower() in best_time.lower():\n",
    "                if month == current_month:\n",
    "                    return True\n",
    "    \n",
    "    # Default to False if we couldn't parse the best time information\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content-based feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_content_features(df):\n",
    "    \"\"\"\n",
    "    Generate content-based features from destination descriptions\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Dataframe containing destination information\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Similarity matrix based on content features\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating content-based features\")\n",
    "    # Combine important features for content-based filtering\n",
    "    df['content_features'] = df['About the city (long Description)'].fillna('') + ' ' + df['Best Time to visit'].fillna('')\n",
    "    \n",
    "    # Create TF-IDF features\n",
    "    tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "    \n",
    "    # Handle potential empty strings\n",
    "    if df['content_features'].str.strip().str.len().eq(0).any():\n",
    "        logger.warning(\"Some destinations have empty descriptions\")\n",
    "        df.loc[df['content_features'].str.strip().str.len().eq(0), 'content_features'] = \"No description available\"\n",
    "    \n",
    "    try:\n",
    "        tfidf_matrix = tfidf.fit_transform(df['content_features'])\n",
    "        \n",
    "        # Calculate similarity matrix\n",
    "        similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "        logger.info(f\"Generated similarity matrix of shape {similarity_matrix.shape}\")\n",
    "        return similarity_matrix\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating content features: {str(e)}\")\n",
    "        # Return identity matrix as fallback\n",
    "        return np.eye(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iternary Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_itinerary(destination, places_data, travel_date, weather_data):\n",
    "    \"\"\"\n",
    "    Generate a travel itinerary for a destination\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    destination : str\n",
    "        Name of the destination\n",
    "    places_data : dict\n",
    "        Dictionary with places and their traffic information\n",
    "    travel_date : datetime\n",
    "        Date of travel\n",
    "    weather_data : dict\n",
    "        Weather information for the destination\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of places to visit with recommended timing\n",
    "    \"\"\"\n",
    "    logger.info(f\"Generating itinerary for {destination}\")\n",
    "    itinerary = []\n",
    "    \n",
    "    # Sort places by traffic - less traffic first\n",
    "    sorted_places = sorted(places_data.items(), key=lambda x: x[1])\n",
    "    \n",
    "    # Weather condition affects outdoor vs indoor activities\n",
    "    weather_condition = weather_data.get(\"condition\", \"unknown\")\n",
    "    outdoor_friendly = (\"clear\" in weather_condition or \n",
    "                        \"sunny\" in weather_condition or \n",
    "                        \"few clouds\" in weather_condition)\n",
    "    \n",
    "    morning_places = []\n",
    "    afternoon_places = []\n",
    "    evening_places = []\n",
    "    \n",
    "    # Distribute places throughout the day based on traffic\n",
    "    for i, (place, travel_time) in enumerate(sorted_places):\n",
    "        if travel_time < 25:  # Low traffic places\n",
    "            best_time = \"Morning\" if outdoor_friendly else \"Afternoon\"\n",
    "            if outdoor_friendly:\n",
    "                morning_places.append((place, travel_time))\n",
    "            else:\n",
    "                afternoon_places.append((place, travel_time))\n",
    "        elif travel_time < 40:  # Medium traffic places\n",
    "            best_time = \"Afternoon\"\n",
    "            afternoon_places.append((place, travel_time))\n",
    "        else:  # High traffic places\n",
    "            best_time = \"Evening\"\n",
    "            evening_places.append((place, travel_time))\n",
    "    \n",
    "    # Build the itinerary\n",
    "    for place_type, places, time_of_day in [\n",
    "        (\"outdoor\" if outdoor_friendly else \"indoor\", morning_places, \"Morning\"),\n",
    "        (\"mixed\", afternoon_places, \"Afternoon\"),\n",
    "        (\"indoor or dining\", evening_places, \"Evening\")\n",
    "    ]:\n",
    "        for place, travel_time in places:\n",
    "            itinerary.append({\n",
    "                \"Place\": place,\n",
    "                \"Travel Time\": f\"{travel_time} mins\",\n",
    "                \"Best Time to Visit\": time_of_day,\n",
    "                \"Type\": place_type\n",
    "            })\n",
    "    \n",
    "    return itinerary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Destination Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_destinations(df, config, weather_data, traffic_data, similarity_matrix=None, num_recommendations=5):\n",
    "    \"\"\"\n",
    "    Recommend destinations based on user preferences and current conditions\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Dataframe with destination information\n",
    "    config : dict\n",
    "        User preferences and configuration\n",
    "    weather_data : dict\n",
    "        Weather information for each city\n",
    "    traffic_data : dict\n",
    "        Traffic information for each city\n",
    "    similarity_matrix : numpy.ndarray, optional\n",
    "        Content-based similarity matrix\n",
    "    num_recommendations : int\n",
    "        Number of recommendations to return\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Dataframe with recommended destinations\n",
    "    \"\"\"\n",
    "    logger.info(f\"Generating {num_recommendations} destination recommendations\")\n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    recommendations = df.copy()\n",
    "    \n",
    "    # Extract user preferences\n",
    "    weather_pref = config[\"weather_preference_score\"]\n",
    "    avoid_crowds = config[\"avoid_crowds\"]\n",
    "    travel_date = config[\"travel_date\"]\n",
    "    \n",
    "    # Update dataframe with weather and traffic data\n",
    "    for city in recommendations['City']:\n",
    "        if city in weather_data:\n",
    "            recommendations.loc[recommendations['City'] == city, 'Weather Quality'] = weather_data[city]['quality']\n",
    "        \n",
    "        if city in traffic_data:\n",
    "            recommendations.loc[recommendations['City'] == city, 'Traffic Level'] = traffic_data[city]['traffic_level']\n",
    "    \n",
    "    # Fill missing values with average\n",
    "    recommendations['Weather Quality'] = recommendations['Weather Quality'].fillna(recommendations['Weather Quality'].mean())\n",
    "    recommendations['Traffic Level'] = recommendations['Traffic Level'].fillna(recommendations['Traffic Level'].mean())\n",
    "    \n",
    "    # Check if destinations are in season\n",
    "    recommendations['In Season'] = recommendations.apply(\n",
    "        lambda x: is_in_season(x, travel_date), axis=1\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Calculate scores\n",
    "    recommendations[\"Weather Score\"] = 10 - abs(recommendations[\"Weather Quality\"] - weather_pref)\n",
    "    \n",
    "    if avoid_crowds:\n",
    "        # Lower traffic is better (inverse scale)\n",
    "        recommendations[\"Traffic Score\"] = 10 - recommendations[\"Traffic Level\"]\n",
    "    else:\n",
    "        # Traffic doesn't matter as much\n",
    "        recommendations[\"Traffic Score\"] = 5\n",
    "    \n",
    "    # Extra points for being in season\n",
    "    recommendations[\"Season Score\"] = recommendations[\"In Season\"] * 2\n",
    "    \n",
    "    # For destinations with ratings (assuming 'Rating' column exists)\n",
    "    if 'Rating' in recommendations.columns:\n",
    "        # Normalize ratings to 0-10 scale if needed\n",
    "        max_rating = recommendations['Rating'].max()\n",
    "        if max_rating > 0:\n",
    "            recommendations[\"Rating Score\"] = (recommendations[\"Rating\"] / max_rating) * 10\n",
    "        else:\n",
    "            recommendations[\"Rating Score\"] = 5\n",
    "    else:\n",
    "        recommendations[\"Rating Score\"] = 5\n",
    "    \n",
    "    # Calculate final score with weighted components\n",
    "    # Prioritize weather and traffic according to user preferences\n",
    "    recommendations[\"Final Score\"] = (\n",
    "        (0.4 * recommendations[\"Weather Score\"]) + \n",
    "        (0.4 * recommendations[\"Traffic Score\"]) + \n",
    "        (0.1 * recommendations[\"Season Score\"]) + \n",
    "        (0.1 * recommendations[\"Rating Score\"])\n",
    "    )\n",
    "    \n",
    "    # Sort by final score\n",
    "    recommendations = recommendations.sort_values(by=\"Final Score\", ascending=False)\n",
    "    \n",
    "    # Return top recommendations\n",
    "    return recommendations.head(num_recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 10:10:51,388 - INFO - Starting Travel Recommendation System\n",
      "2025-02-27 10:10:51,396 - INFO - Removed 13 duplicate records\n",
      "2025-02-27 10:10:51,399 - INFO - âœ… Dataset Loaded with 58 records.\n",
      "2025-02-27 10:10:51,400 - INFO - Processing 58 cities\n",
      "2025-02-27 10:10:51,403 - INFO - Generating new weather data\n",
      "2025-02-27 10:10:51,405 - INFO - Generating new traffic data\n",
      "2025-02-27 10:10:51,408 - INFO - Starting feature engineering...\n",
      "2025-02-27 10:10:51,757 - INFO - Class distribution - Good: 5, Not Good: 53\n",
      "2025-02-27 10:10:51,758 - INFO - Feature engineering completed in 0.35 seconds\n",
      "2025-02-27 10:10:51,767 - INFO - Dropping highly correlated features: ['Weather_Traffic_Interaction', 'Num Attractions']\n",
      "2025-02-27 10:10:51,768 - INFO - Selected 14 features for model training\n",
      "2025-02-27 10:10:51,770 - INFO - ðŸš€ Training models using 14 features\n",
      "2025-02-27 10:10:51,773 - WARNING - Class imbalance detected: [53  5]\n",
      "2025-02-27 10:10:51,779 - INFO - Small dataset detected, simplifying parameter grid\n",
      "2025-02-27 10:10:51,780 - INFO - Training Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 10:10:53,352 - INFO -   Random Forest - Accuracy: 1.0000, F1: 1.0000, Time: 1.57s\n",
      "2025-02-27 10:10:53,352 - INFO - Training Gradient Boosting...\n",
      "2025-02-27 10:10:53,641 - INFO -   Gradient Boosting - Accuracy: 0.9167, F1: 0.9286, Time: 0.29s\n",
      "2025-02-27 10:10:53,641 - INFO - \n",
      "ðŸ” Feature Importance for Best Model:\n",
      "2025-02-27 10:10:53,641 - INFO -   Weather Quality: 0.3251\n",
      "2025-02-27 10:10:53,641 - INFO -   Traffic Level: 0.1658\n",
      "2025-02-27 10:10:53,653 - INFO -   Year_round: 0.0161\n",
      "2025-02-27 10:10:53,655 - INFO -   Temperature: 0.1038\n",
      "2025-02-27 10:10:53,655 - INFO -   Temp_Comfort: 0.1059\n",
      "2025-02-27 10:10:53,658 - INFO -   Attractions_Score: 0.0657\n",
      "2025-02-27 10:10:53,658 - INFO -   Weather_Condition_Clear: 0.0154\n",
      "2025-02-27 10:10:53,658 - INFO -   Weather_Condition_Cloudy: 0.0093\n",
      "2025-02-27 10:10:53,658 - INFO -   Weather_Condition_Rainy: 0.0080\n",
      "2025-02-27 10:10:53,666 - INFO -   Weather_Condition_Sunny: 0.0733\n",
      "2025-02-27 10:10:53,667 - INFO -   City_Type_metropolitan: 0.0642\n",
      "2025-02-27 10:10:53,670 - INFO -   City_Type_rural: 0.0157\n",
      "2025-02-27 10:10:53,674 - INFO -   City_Type_suburban: 0.0162\n",
      "2025-02-27 10:10:53,674 - INFO -   City_Type_urban: 0.0155\n",
      "2025-02-27 10:10:53,677 - INFO - Most important features: [('Weather Quality', 0.32505856850094306), ('Traffic Level', 0.16584033689210265), ('Temp_Comfort', 0.1058747704506133)]\n",
      "2025-02-27 10:10:53,677 - INFO - Model training completed in 1.91 seconds\n",
      "2025-02-27 10:10:53,708 - INFO - âœ… Enhanced Model Saved as travel_recommendation_enhanced.pkl\n",
      "2025-02-27 10:10:53,909 - INFO - âœ… Enhanced Model also saved as travel_recommendation_enhanced.joblib\n",
      "2025-02-27 10:10:53,910 - INFO - \n",
      "ðŸŽ¯ Generating Smart Recommendations...\n",
      "2025-02-27 10:10:53,912 - INFO - Starting feature engineering...\n",
      "2025-02-27 10:10:54,083 - INFO - Class distribution - Good: 5, Not Good: 53\n",
      "2025-02-27 10:10:54,084 - INFO - Feature engineering completed in 0.17 seconds\n",
      "C:\\Users\\sanjay\\AppData\\Local\\Temp\\ipykernel_10640\\4154226559.py:524: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  detailed_recommendations[col] = detailed_recommendations[col].round(2)\n",
      "2025-02-27 10:10:54,124 - INFO - \n",
      "ðŸ™ï¸ Top 5 Recommended Destinations:\n",
      "\n",
      "2025-02-27 10:10:54,124 - INFO -                     City  Weather Quality  Traffic Level  \\\n",
      "39             Bangalore              9.0            4.0   \n",
      "45                  Pune             10.0            2.0   \n",
      "56             Thanjavur             10.0            2.0   \n",
      "42  Shimoga (Shivamogga)             10.0            5.0   \n",
      "30               Kolkata              9.0            2.0   \n",
      "\n",
      "    Recommendation Score  Final Score  Year_round  Temp_Comfort  \\\n",
      "39                  0.78         8.16           1          10.0   \n",
      "45                  0.79         8.03           0           9.0   \n",
      "56                  0.71         7.85           0           8.5   \n",
      "42                  0.88         7.46           0           9.0   \n",
      "30                  0.68         7.39           0           8.5   \n",
      "\n",
      "    Attractions_Score  Num Attractions  Temperature  \n",
      "39               5.33              8.0         24.0  \n",
      "45               4.67              7.0         22.0  \n",
      "56               1.33              2.0         27.0  \n",
      "42               6.67             10.0         26.0  \n",
      "30               2.67              4.0         27.0  \n",
      "2025-02-27 10:10:54,142 - INFO - \n",
      "ðŸ“Š Recommendation Analysis:\n",
      "2025-02-27 10:10:54,143 - INFO -   Best destination: Bangalore with score 8.16/10\n",
      "2025-02-27 10:10:54,146 - INFO -   Most weather-friendly: Thanjavur\n",
      "2025-02-27 10:10:54,148 - INFO -   Least crowded: Thanjavur\n",
      "2025-02-27 10:10:54,150 - INFO - Total execution time: 2.76 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import joblib\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.FileHandler(\"model_training.log\"), logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# API Keys - Store in environment variables for better security\n",
    "API_KEY_WEATHER = os.environ.get('WEATHER_API_KEY', \"ae17ccc0bd407c2a7a09e95fa78d1d2d\")\n",
    "\n",
    "# Load Dataset with better error handling\n",
    "def load_data(file_path=\"holidify.csv\"):\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            logger.error(f\"File not found: {file_path}\")\n",
    "            return None\n",
    "            \n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check if required columns exist\n",
    "        required_columns = ['City', 'Best Time to visit']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            logger.error(f\"Missing required columns: {missing_columns}\")\n",
    "            return None\n",
    "            \n",
    "        # Drop duplicates and clean the data\n",
    "        original_count = len(df)\n",
    "        df = df.drop_duplicates(subset=['City'])\n",
    "        logger.info(f\"Removed {original_count - len(df)} duplicate records\")\n",
    "        \n",
    "        df['City'] = df['City'].str.strip()\n",
    "        \n",
    "        # Fill missing values in categorical columns\n",
    "        missing_best_time = df['Best Time to visit'].isna().sum()\n",
    "        if missing_best_time > 0:\n",
    "            logger.info(f\"Filling {missing_best_time} missing values in 'Best Time to visit'\")\n",
    "            df['Best Time to visit'] = df['Best Time to visit'].fillna('Throughout the year')\n",
    "        \n",
    "        # Add columns for feature engineering\n",
    "        df['Year_round'] = df['Best Time to visit'].str.contains('Throughout the year').astype(int)\n",
    "        \n",
    "        logger.info(f\"âœ… Dataset Loaded with {len(df)} records.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Error loading data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Cache for weather and traffic data to avoid regenerating during testing\n",
    "_weather_cache = {}\n",
    "_traffic_cache = {}\n",
    "\n",
    "# Enhanced Weather Data Generation with more realistic distributions and caching\n",
    "def get_enhanced_weather_data(cities, use_cache=True):\n",
    "    global _weather_cache\n",
    "    \n",
    "    # Return cached data if available and requested\n",
    "    if use_cache and _weather_cache and set(cities).issubset(set(_weather_cache.keys())):\n",
    "        logger.info(\"Using cached weather data\")\n",
    "        return {city: _weather_cache[city] for city in cities}\n",
    "    \n",
    "    logger.info(\"Generating new weather data\")\n",
    "    weather_data = {}\n",
    "    # More realistic weather conditions with seasonal variation\n",
    "    conditions = {\n",
    "        \"Clear\": {\"quality_range\": (7, 10), \"prob\": 0.4, \"temp_range\": (18, 28)},\n",
    "        \"Cloudy\": {\"quality_range\": (5, 8), \"prob\": 0.3, \"temp_range\": (15, 25)},\n",
    "        \"Rainy\": {\"quality_range\": (3, 6), \"prob\": 0.2, \"temp_range\": (12, 22)},\n",
    "        \"Sunny\": {\"quality_range\": (8, 10), \"prob\": 0.1, \"temp_range\": (22, 35)}\n",
    "    }\n",
    "    \n",
    "    # Use a fixed seed for reproducibility during testing\n",
    "    random.seed(42)\n",
    "    \n",
    "    for city in cities:\n",
    "        # Use weighted random choice for condition\n",
    "        condition = random.choices(\n",
    "            list(conditions.keys()), \n",
    "            weights=[cond['prob'] for cond in conditions.values()], \n",
    "            k=1\n",
    "        )[0]\n",
    "        \n",
    "        # Get quality range for the selected condition\n",
    "        quality_range = conditions[condition][\"quality_range\"]\n",
    "        temp_range = conditions[condition][\"temp_range\"]\n",
    "        \n",
    "        quality = random.randint(quality_range[0], quality_range[1])\n",
    "        \n",
    "        weather_data[city] = {\n",
    "            \"quality\": quality,\n",
    "            \"condition\": condition,\n",
    "            \"temp\": random.randint(temp_range[0], temp_range[1])\n",
    "        }\n",
    "    \n",
    "    # Update cache\n",
    "    _weather_cache.update(weather_data)\n",
    "    \n",
    "    # Reset random seed\n",
    "    random.seed(None)\n",
    "    \n",
    "    return weather_data\n",
    "\n",
    "# Enhanced Traffic Data with seasonal variation and caching\n",
    "def get_enhanced_traffic_data(cities, use_cache=True):\n",
    "    global _traffic_cache\n",
    "    \n",
    "    # Return cached data if available and requested\n",
    "    if use_cache and _traffic_cache and set(cities).issubset(set(_traffic_cache.keys())):\n",
    "        logger.info(\"Using cached traffic data\")\n",
    "        return {city: _traffic_cache[city] for city in cities}\n",
    "    \n",
    "    logger.info(\"Generating new traffic data\")\n",
    "    traffic_data = {}\n",
    "    \n",
    "    # Define different city types with different traffic distributions\n",
    "    city_types = {\n",
    "        \"metropolitan\": {\"traffic_range\": (6, 10), \"prob\": 0.3, \"attractions\": (8, 15)},\n",
    "        \"urban\": {\"traffic_range\": (4, 8), \"prob\": 0.4, \"attractions\": (6, 10)},\n",
    "        \"suburban\": {\"traffic_range\": (2, 6), \"prob\": 0.2, \"attractions\": (4, 8)},\n",
    "        \"rural\": {\"traffic_range\": (1, 4), \"prob\": 0.1, \"attractions\": (2, 5)}\n",
    "    }\n",
    "    \n",
    "    # Use a fixed seed for reproducibility during testing\n",
    "    random.seed(42)\n",
    "    \n",
    "    for city in cities:\n",
    "        # Assign city type based on probability\n",
    "        city_type = random.choices(\n",
    "            list(city_types.keys()), \n",
    "            weights=[type_data['prob'] for type_data in city_types.values()], \n",
    "            k=1\n",
    "        )[0]\n",
    "        \n",
    "        traffic_range = city_types[city_type][\"traffic_range\"]\n",
    "        base_traffic = random.randint(traffic_range[0], traffic_range[1])\n",
    "        \n",
    "        # Number of attractions based on city type\n",
    "        num_attractions = random.randint(*city_types[city_type][\"attractions\"])\n",
    "        \n",
    "        # Create more realistic travel times for attractions based on traffic level\n",
    "        places = {}\n",
    "        for i in range(1, num_attractions + 1):\n",
    "            # More congested cities have longer travel times with higher variance\n",
    "            min_time = 10 + (base_traffic * 2)\n",
    "            max_time = 20 + (base_traffic * 5)\n",
    "            places[f\"{city} Place {i}\"] = random.randint(min_time, max_time)\n",
    "        \n",
    "        traffic_data[city] = {\n",
    "            \"traffic_level\": base_traffic,\n",
    "            \"city_type\": city_type,\n",
    "            \"places\": places,\n",
    "            \"num_attractions\": num_attractions\n",
    "        }\n",
    "    \n",
    "    # Update cache\n",
    "    _traffic_cache.update(traffic_data)\n",
    "    \n",
    "    # Reset random seed\n",
    "    random.seed(None)\n",
    "    \n",
    "    return traffic_data\n",
    "\n",
    "# Feature Engineering Function with more advanced features\n",
    "def engineer_features(df, weather_data, traffic_data):\n",
    "    \"\"\"Create additional features for better predictions\"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info(\"Starting feature engineering...\")\n",
    "    \n",
    "    # Create copies of data to avoid modifying original\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Merge Weather & Traffic Data\n",
    "    for city in df_processed[\"City\"]:\n",
    "        if city in weather_data:\n",
    "            df_processed.loc[df_processed[\"City\"] == city, \"Weather Quality\"] = weather_data[city][\"quality\"]\n",
    "            df_processed.loc[df_processed[\"City\"] == city, \"Temperature\"] = weather_data[city].get(\"temp\", 25)\n",
    "            df_processed.loc[df_processed[\"City\"] == city, \"Weather Condition\"] = weather_data[city].get(\"condition\", \"Unknown\")\n",
    "        if city in traffic_data:\n",
    "            df_processed.loc[df_processed[\"City\"] == city, \"Traffic Level\"] = traffic_data[city][\"traffic_level\"]\n",
    "            df_processed.loc[df_processed[\"City\"] == city, \"City Type\"] = traffic_data[city].get(\"city_type\", \"urban\")\n",
    "            df_processed.loc[df_processed[\"City\"] == city, \"Num Attractions\"] = traffic_data[city].get(\"num_attractions\", 5)\n",
    "\n",
    "    # Fill Missing Values with more intelligent defaults based on similar cities\n",
    "    # For weather quality, group by weather condition\n",
    "    if \"Weather Condition\" in df_processed.columns and \"Weather Quality\" in df_processed.columns:\n",
    "        condition_medians = df_processed.groupby(\"Weather Condition\")[\"Weather Quality\"].median().to_dict()\n",
    "        \n",
    "        # For each missing value, fill with the median of that condition\n",
    "        for condition, median in condition_medians.items():\n",
    "            mask = (df_processed[\"Weather Quality\"].isna()) & (df_processed[\"Weather Condition\"] == condition)\n",
    "            df_processed.loc[mask, \"Weather Quality\"] = median\n",
    "    \n",
    "    # Fill remaining missing values with medians\n",
    "    for col in [\"Weather Quality\", \"Traffic Level\", \"Temperature\", \"Num Attractions\"]:\n",
    "        if col in df_processed.columns:\n",
    "            missing_count = df_processed[col].isna().sum()\n",
    "            if missing_count > 0:\n",
    "                logger.info(f\"Filling {missing_count} missing values in '{col}'\")\n",
    "                df_processed[col] = df_processed[col].fillna(df_processed[col].median())\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    categorical_cols = [\"Weather Condition\", \"City Type\"]\n",
    "    for col in categorical_cols:\n",
    "        if col in df_processed.columns:\n",
    "            df_processed = pd.get_dummies(df_processed, columns=[col], prefix=col.replace(\" \", \"_\"))\n",
    "    \n",
    "    # Create interaction features\n",
    "    df_processed[\"Weather_Traffic_Interaction\"] = df_processed[\"Weather Quality\"] * (10 - df_processed[\"Traffic Level\"]) / 10\n",
    "    \n",
    "    # Temperature comfort score (optimal around 24-26Â°C)\n",
    "    if \"Temperature\" in df_processed.columns:\n",
    "        df_processed[\"Temp_Comfort\"] = 10 - abs(df_processed[\"Temperature\"] - 24) / 2\n",
    "        df_processed[\"Temp_Comfort\"] = df_processed[\"Temp_Comfort\"].clip(0, 10)\n",
    "    \n",
    "    # Attractions density score\n",
    "    if \"Num Attractions\" in df_processed.columns:\n",
    "        max_attractions = df_processed[\"Num Attractions\"].max()\n",
    "        df_processed[\"Attractions_Score\"] = df_processed[\"Num Attractions\"] / max_attractions * 10\n",
    "    \n",
    "    # Create target variable with more nuanced definition\n",
    "    df_processed[\"Destination Score\"] = (\n",
    "        (0.35 * df_processed[\"Weather Quality\"]) + \n",
    "        (0.30 * (10 - df_processed[\"Traffic Level\"])) + \n",
    "        (0.15 * df_processed[\"Year_round\"]) +\n",
    "        (0.10 * df_processed.get(\"Temp_Comfort\", 5)) +\n",
    "        (0.10 * df_processed.get(\"Attractions_Score\", 5))\n",
    "    )\n",
    "    \n",
    "    # Create classification target\n",
    "    df_processed[\"Good Destination\"] = (df_processed[\"Destination Score\"] >= 6.5).astype(int)\n",
    "    \n",
    "    # Log class balance\n",
    "    class_counts = df_processed[\"Good Destination\"].value_counts()\n",
    "    logger.info(f\"Class distribution - Good: {class_counts.get(1, 0)}, Not Good: {class_counts.get(0, 0)}\")\n",
    "    \n",
    "    logger.info(f\"Feature engineering completed in {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Perform feature selection to improve model performance\n",
    "def select_features(df_processed):\n",
    "    \"\"\"Select most important features to reduce dimensionality\"\"\"\n",
    "    # Basic features we always want to include\n",
    "    base_features = [\"Weather Quality\", \"Traffic Level\", \"Year_round\"]\n",
    "    \n",
    "    # Additional features if they exist\n",
    "    additional_features = [\n",
    "        \"Temperature\", \"Weather_Traffic_Interaction\", \n",
    "        \"Temp_Comfort\", \"Attractions_Score\", \"Num Attractions\"\n",
    "    ]\n",
    "    \n",
    "    # One-hot encoded features\n",
    "    weather_condition_features = [col for col in df_processed.columns if col.startswith(\"Weather_Condition_\")]\n",
    "    city_type_features = [col for col in df_processed.columns if col.startswith(\"City_Type_\")]\n",
    "    \n",
    "    # Combine all potential features\n",
    "    potential_features = (\n",
    "        base_features + \n",
    "        [f for f in additional_features if f in df_processed.columns] +\n",
    "        weather_condition_features +\n",
    "        city_type_features\n",
    "    )\n",
    "    \n",
    "    # Check for high correlation between features\n",
    "    if len(potential_features) > 1:\n",
    "        corr_matrix = df_processed[potential_features].corr().abs()\n",
    "        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        \n",
    "        # Find features with correlation > 0.85\n",
    "        to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.85)]\n",
    "        \n",
    "        if to_drop:\n",
    "            logger.info(f\"Dropping highly correlated features: {to_drop}\")\n",
    "            potential_features = [f for f in potential_features if f not in to_drop]\n",
    "    \n",
    "    logger.info(f\"Selected {len(potential_features)} features for model training\")\n",
    "    return potential_features\n",
    "\n",
    "# Train Multiple Models with cross-validation and Select Best\n",
    "def train_recommendation_models(df_processed, features):\n",
    "    start_time = time.time()\n",
    "    logger.info(f\"ðŸš€ Training models using {len(features)} features\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df_processed[features].values\n",
    "    y = df_processed[\"Good Destination\"].values\n",
    "    \n",
    "    # Check for class imbalance\n",
    "    class_counts = np.bincount(y)\n",
    "    if len(class_counts) > 1 and min(class_counts) / sum(class_counts) < 0.2:\n",
    "        logger.warning(f\"Class imbalance detected: {class_counts}\")\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Split data with stratification to handle imbalanced classes\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Model dictionary with optimized parameters\n",
    "    models = {\n",
    "        \"Random Forest\": RandomForestClassifier(\n",
    "            random_state=42,\n",
    "            n_jobs=-1,  # Use all available cores\n",
    "            class_weight='balanced' if len(class_counts) > 1 and min(class_counts) / sum(class_counts) < 0.3 else None\n",
    "        ),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(\n",
    "            random_state=42,\n",
    "            validation_fraction=0.1,\n",
    "            n_iter_no_change=5,\n",
    "            tol=1e-4\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Parameter grid for GridSearchCV\n",
    "    param_grids = {\n",
    "        \"Random Forest\": {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 2],\n",
    "            'max_features': ['sqrt', 'log2']\n",
    "        },\n",
    "        \"Gradient Boosting\": {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.05, 0.1],\n",
    "            'max_depth': [3, 5],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'max_features': ['sqrt', 'log2']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    results = {}\n",
    "    \n",
    "    # For small datasets, adjust the param grids\n",
    "    if len(X_train) < 200:\n",
    "        logger.info(\"Small dataset detected, simplifying parameter grid\")\n",
    "        for model_name in param_grids:\n",
    "            for param in param_grids[model_name]:\n",
    "                param_grids[model_name][param] = param_grids[model_name][param][:1] # Just use first value\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for name, model in models.items():\n",
    "        model_start_time = time.time()\n",
    "        logger.info(f\"Training {name}...\")\n",
    "        \n",
    "        # Use GridSearchCV for hyperparameter tuning\n",
    "        grid_search = GridSearchCV(\n",
    "            model, param_grids[name], cv=min(5, len(X_train) // 10) if len(X_train) < 50 else 5, \n",
    "            scoring='f1_weighted', n_jobs=-1 if len(X_train) > 1000 else 1\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model_params = grid_search.best_estimator_\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            y_pred = best_model_params.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "            \n",
    "            # Generate detailed classification report\n",
    "            report = classification_report(y_test, y_pred, target_names=['Not Good', 'Good'], output_dict=True)\n",
    "            \n",
    "            # Get feature importance\n",
    "            if hasattr(best_model_params, 'feature_importances_'):\n",
    "                feature_importance = best_model_params.feature_importances_\n",
    "            else:\n",
    "                feature_importance = np.zeros(len(features))\n",
    "            \n",
    "            results[name] = {\n",
    "                'model': best_model_params,\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'feature_importance': feature_importance,\n",
    "                'best_params': grid_search.best_params_,\n",
    "                'classification_report': report\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"  {name} - Accuracy: {accuracy:.4f}, F1: {f1:.4f}, \"\n",
    "                      f\"Time: {time.time() - model_start_time:.2f}s\")\n",
    "            \n",
    "            # Track best model\n",
    "            if f1 > best_score:\n",
    "                best_score = f1\n",
    "                best_model = best_model_params\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error training {name}: {str(e)}\")\n",
    "    \n",
    "    if not best_model:\n",
    "        logger.error(\"Failed to train any models successfully\")\n",
    "        return None, None, features, scaler\n",
    "    \n",
    "    # Print feature importance for the best model\n",
    "    logger.info(\"\\nðŸ” Feature Importance for Best Model:\")\n",
    "    best_model_name = max(results, key=lambda k: results[k]['f1'])\n",
    "    importance = results[best_model_name]['feature_importance']\n",
    "    \n",
    "    feature_importance_dict = {}\n",
    "    for i, feat in enumerate(features):\n",
    "        logger.info(f\"  {feat}: {importance[i]:.4f}\")\n",
    "        feature_importance_dict[feat] = importance[i]\n",
    "    \n",
    "    # Sort features by importance\n",
    "    sorted_features = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    logger.info(f\"Most important features: {sorted_features[:3]}\")\n",
    "    \n",
    "    logger.info(f\"Model training completed in {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    return best_model, results, features, scaler\n",
    "\n",
    "# Save Model with all necessary components\n",
    "def save_enhanced_model(model, features, scaler, results=None, filename=\"travel_recommendation_enhanced.pkl\"):\n",
    "    if model is None:\n",
    "        logger.error(\"No model to save\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        model_data = {\n",
    "            \"model\": model,\n",
    "            \"features\": features,\n",
    "            \"scaler\": scaler,\n",
    "            \"version\": \"2.0\",\n",
    "            \"training_date\": datetime.now(),\n",
    "            \"results\": results\n",
    "        }\n",
    "        \n",
    "        with open(filename, \"wb\") as file:\n",
    "            pickle.dump(model_data, file)\n",
    "        \n",
    "        logger.info(f\"âœ… Enhanced Model Saved as {filename}\")\n",
    "        \n",
    "        # Also save as joblib file for faster loading\n",
    "        joblib_filename = filename.replace('.pkl', '.joblib')\n",
    "        joblib.dump(model_data, joblib_filename)\n",
    "        logger.info(f\"âœ… Enhanced Model also saved as {joblib_filename}\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving model: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Generate Recommendations using the trained model\n",
    "def generate_smart_recommendations(df, model, weather_data, traffic_data, features, scaler, num_recommendations=5):\n",
    "    if model is None:\n",
    "        logger.error(\"No model available for generating recommendations\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(\"\\nðŸŽ¯ Generating Smart Recommendations...\")\n",
    "    \n",
    "    try:\n",
    "        # Engineer features for all cities\n",
    "        df_processed = engineer_features(df, weather_data, traffic_data)\n",
    "        \n",
    "        # Check if all required features exist\n",
    "        missing_features = [f for f in features if f not in df_processed.columns]\n",
    "        if missing_features:\n",
    "            logger.warning(f\"Missing features in processed data: {missing_features}\")\n",
    "            # Create missing features with default values\n",
    "            for feature in missing_features:\n",
    "                df_processed[feature] = 0\n",
    "        \n",
    "        # Prepare feature matrix\n",
    "        X = df_processed[features].values\n",
    "        X_scaled = scaler.transform(X)\n",
    "        \n",
    "        # Get model predictions\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            # If model supports probability, use it for ranking\n",
    "            proba = model.predict_proba(X_scaled)\n",
    "            df_processed[\"Recommendation Score\"] = proba[:, 1]  # Probability of being a good destination\n",
    "        else:\n",
    "            # Fall back to decision function or manual scoring\n",
    "            df_processed[\"Recommendation Score\"] = model.predict(X_scaled).astype(float)\n",
    "        \n",
    "        # Calculate a weighted final score with confidence adjustment\n",
    "        df_processed[\"Final Score\"] = (\n",
    "            (0.40 * df_processed[\"Weather Quality\"] / 10) +  # Normalize to 0-1\n",
    "            (0.25 * (10 - df_processed[\"Traffic Level\"]) / 10) +  # Inverse and normalize\n",
    "            (0.20 * df_processed[\"Recommendation Score\"]) +  # Already 0-1\n",
    "            (0.10 * df_processed[\"Year_round\"]) +  # 0 or 1\n",
    "            (0.05 * df_processed.get(\"Temp_Comfort\", 5) / 10)  # Normalize to 0-1\n",
    "        ) * 10  # Scale to 0-10\n",
    "        \n",
    "        # Get top recommendations\n",
    "        recommendations = df_processed.sort_values(by=\"Final Score\", ascending=False).head(num_recommendations)\n",
    "        \n",
    "        # Create a more detailed recommendation output\n",
    "        columns_to_include = [\"City\", \"Weather Quality\", \"Traffic Level\", \n",
    "                             \"Recommendation Score\", \"Final Score\", \"Year_round\"]\n",
    "        \n",
    "        optional_columns = [\"Temp_Comfort\", \"Attractions_Score\", \"Num Attractions\", \"Temperature\"]\n",
    "        for col in optional_columns:\n",
    "            if col in df_processed.columns:\n",
    "                columns_to_include.append(col)\n",
    "        \n",
    "        detailed_recommendations = recommendations[columns_to_include]\n",
    "        \n",
    "        # Round scores for cleaner display\n",
    "        numeric_columns = detailed_recommendations.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_columns:\n",
    "            detailed_recommendations[col] = detailed_recommendations[col].round(2)\n",
    "        \n",
    "        return detailed_recommendations\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating recommendations: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Main Execution Pipeline with enhanced process and error handling\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    logger.info(\"Starting Travel Recommendation System\")\n",
    "    \n",
    "    df = load_data()\n",
    "    if df is None:\n",
    "        logger.error(\"Failed to load data. Exiting.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        cities = df[\"City\"].tolist()\n",
    "        logger.info(f\"Processing {len(cities)} cities\")\n",
    "        \n",
    "        # Generate enhanced data\n",
    "        weather_data = get_enhanced_weather_data(cities)\n",
    "        traffic_data = get_enhanced_traffic_data(cities)\n",
    "        \n",
    "        # Engineer features\n",
    "        df_processed = engineer_features(df, weather_data, traffic_data)\n",
    "        \n",
    "        # Select most important features\n",
    "        selected_features = select_features(df_processed)\n",
    "        \n",
    "        # Train and select best model\n",
    "        best_model, model_results, features, scaler = train_recommendation_models(df_processed, selected_features)\n",
    "        \n",
    "        if best_model is None:\n",
    "            logger.error(\"Model training failed. Exiting.\")\n",
    "            return False\n",
    "        \n",
    "        # Save the enhanced model with all components\n",
    "        save_success = save_enhanced_model(best_model, features, scaler, model_results)\n",
    "        \n",
    "        if not save_success:\n",
    "            logger.warning(\"Failed to save model\")\n",
    "        \n",
    "        # Generate recommendations\n",
    "        recommendations = generate_smart_recommendations(\n",
    "            df, best_model, weather_data, traffic_data, features, scaler\n",
    "        )\n",
    "        \n",
    "        if recommendations is None:\n",
    "            logger.error(\"Failed to generate recommendations\")\n",
    "            return False\n",
    "        \n",
    "        logger.info(\"\\nðŸ™ï¸ Top 5 Recommended Destinations:\\n\")\n",
    "        logger.info(recommendations)\n",
    "        \n",
    "        # Visualize the top destinations\n",
    "        logger.info(\"\\nðŸ“Š Recommendation Analysis:\")\n",
    "        logger.info(f\"  Best destination: {recommendations.iloc[0]['City']} with score {recommendations.iloc[0]['Final Score']:.2f}/10\")\n",
    "        \n",
    "        weather_friendly = df_processed.sort_values('Weather Quality', ascending=False)['City'].values[0]\n",
    "        logger.info(f\"  Most weather-friendly: {weather_friendly}\")\n",
    "        \n",
    "        least_crowded = df_processed.sort_values('Traffic Level')['City'].values[0]\n",
    "        logger.info(f\"  Least crowded: {least_crowded}\")\n",
    "        \n",
    "        logger.info(f\"Total execution time: {time.time() - start_time:.2f} seconds\")\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 10:32:34,181 - INFO - Starting Travel Recommendation System\n",
      "2025-02-27 10:32:34,206 - INFO - Removed 13 duplicate records\n",
      "2025-02-27 10:32:34,214 - INFO - âœ… Dataset Loaded with 58 records.\n",
      "2025-02-27 10:32:34,216 - INFO - Processing 58 cities\n",
      "2025-02-27 10:32:34,217 - INFO - Generating new weather data\n",
      "2025-02-27 10:32:34,219 - INFO - Generating new traffic data\n",
      "2025-02-27 10:32:34,221 - INFO - Starting feature engineering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 10:32:34,414 - INFO - Class distribution - Good: 5, Not Good: 53\n",
      "2025-02-27 10:32:34,414 - INFO - Feature engineering completed in 0.19 seconds\n",
      "2025-02-27 10:32:34,422 - INFO - Dropping highly correlated features: ['Weather_Traffic_Interaction', 'Num Attractions']\n",
      "2025-02-27 10:32:34,422 - INFO - Selected 14 features for model training\n",
      "2025-02-27 10:32:34,424 - INFO - ðŸš€ Training models using 14 features\n",
      "2025-02-27 10:32:34,430 - WARNING - Class imbalance detected: [53  5]\n",
      "2025-02-27 10:32:34,437 - INFO - Small dataset detected, simplifying parameter grid\n",
      "2025-02-27 10:32:34,437 - INFO - Training Random Forest...\n",
      "2025-02-27 10:32:35,869 - INFO -   Random Forest - Accuracy: 1.0000, F1: 1.0000, Time: 1.43s\n",
      "2025-02-27 10:32:35,870 - INFO - Training Gradient Boosting...\n",
      "2025-02-27 10:32:36,352 - INFO -   Gradient Boosting - Accuracy: 0.9167, F1: 0.9286, Time: 0.48s\n",
      "2025-02-27 10:32:36,353 - INFO - \n",
      "ðŸ” Feature Importance for Best Model:\n",
      "2025-02-27 10:32:36,353 - INFO -   Weather Quality: 0.3251\n",
      "2025-02-27 10:32:36,355 - INFO -   Traffic Level: 0.1658\n",
      "2025-02-27 10:32:36,357 - INFO -   Year_round: 0.0161\n",
      "2025-02-27 10:32:36,359 - INFO -   Temperature: 0.1038\n",
      "2025-02-27 10:32:36,362 - INFO -   Temp_Comfort: 0.1059\n",
      "2025-02-27 10:32:36,363 - INFO -   Attractions_Score: 0.0657\n",
      "2025-02-27 10:32:36,364 - INFO -   Weather_Condition_Clear: 0.0154\n",
      "2025-02-27 10:32:36,365 - INFO -   Weather_Condition_Cloudy: 0.0093\n",
      "2025-02-27 10:32:36,366 - INFO -   Weather_Condition_Rainy: 0.0080\n",
      "2025-02-27 10:32:36,368 - INFO -   Weather_Condition_Sunny: 0.0733\n",
      "2025-02-27 10:32:36,368 - INFO -   City_Type_metropolitan: 0.0642\n",
      "2025-02-27 10:32:36,369 - INFO -   City_Type_rural: 0.0157\n",
      "2025-02-27 10:32:36,371 - INFO -   City_Type_suburban: 0.0162\n",
      "2025-02-27 10:32:36,372 - INFO -   City_Type_urban: 0.0155\n",
      "2025-02-27 10:32:36,374 - INFO - Most important features: [('Weather Quality', 0.32505856850094306), ('Traffic Level', 0.16584033689210265), ('Temp_Comfort', 0.1058747704506133)]\n",
      "2025-02-27 10:32:36,378 - INFO - Model training completed in 1.95 seconds\n",
      "2025-02-27 10:32:36,401 - INFO - âœ… Enhanced Model Saved as travel_recommendation_enhanced.pkl\n",
      "2025-02-27 10:32:36,506 - INFO - âœ… Enhanced Model also saved as travel_recommendation_enhanced.joblib\n",
      "2025-02-27 10:32:36,508 - INFO - \n",
      "ðŸŽ¯ Generating Smart Recommendations...\n",
      "2025-02-27 10:32:36,511 - INFO - Starting feature engineering...\n",
      "2025-02-27 10:32:36,709 - INFO - Class distribution - Good: 5, Not Good: 53\n",
      "2025-02-27 10:32:36,727 - INFO - Feature engineering completed in 0.22 seconds\n",
      "C:\\Users\\sanjay\\AppData\\Local\\Temp\\ipykernel_10640\\4137804695.py:553: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  detailed_recommendations[col] = detailed_recommendations[col].round(2)\n",
      "2025-02-27 10:32:36,767 - INFO - \n",
      "ðŸ™ï¸ Top 5 Recommended Destinations:\n",
      "\n",
      "2025-02-27 10:32:36,769 - INFO -                     City  Weather Quality  Traffic Level  \\\n",
      "39             Bangalore              9.0            4.0   \n",
      "45                  Pune             10.0            2.0   \n",
      "56             Thanjavur             10.0            2.0   \n",
      "42  Shimoga (Shivamogga)             10.0            5.0   \n",
      "30               Kolkata              9.0            2.0   \n",
      "\n",
      "    Recommendation Score  Final Score  Year_round  Temp_Comfort  \\\n",
      "39                  0.78         8.16           1          10.0   \n",
      "45                  0.79         8.03           0           9.0   \n",
      "56                  0.71         7.85           0           8.5   \n",
      "42                  0.88         7.46           0           9.0   \n",
      "30                  0.68         7.39           0           8.5   \n",
      "\n",
      "    Attractions_Score  Num Attractions  Temperature  \n",
      "39               5.33              8.0         24.0  \n",
      "45               4.67              7.0         22.0  \n",
      "56               1.33              2.0         27.0  \n",
      "42               6.67             10.0         26.0  \n",
      "30               2.67              4.0         27.0  \n",
      "2025-02-27 10:32:36,783 - INFO - \n",
      "ðŸ“Š Recommendation Analysis:\n",
      "2025-02-27 10:32:36,785 - INFO -   Best destination: Bangalore with score 8.16/10\n",
      "2025-02-27 10:32:36,788 - INFO -   Most weather-friendly: Thanjavur\n",
      "2025-02-27 10:32:36,790 - INFO -   Least crowded: Thanjavur\n",
      "2025-02-27 10:32:36,794 - INFO - Total execution time: 2.61 seconds\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model -2 (User_Preferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import joblib\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.FileHandler(\"model_training.log\"), logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# API Keys - Store in environment variables for better security\n",
    "API_KEY_WEATHER = os.environ.get('WEATHER_API_KEY', \"ae17ccc0bd407c2a7a09e95fa78d1d2d\")\n",
    "\n",
    "# Load Dataset with better error handling\n",
    "def load_data(file_path=\"holidify.csv\"):\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            logger.error(f\"File not found: {file_path}\")\n",
    "            return None\n",
    "            \n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check if required columns exist\n",
    "        required_columns = ['City', 'Best Time to visit']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            logger.error(f\"Missing required columns: {missing_columns}\")\n",
    "            return None\n",
    "            \n",
    "        # Drop duplicates and clean the data\n",
    "        original_count = len(df)\n",
    "        df = df.drop_duplicates(subset=['City'])\n",
    "        logger.info(f\"Removed {original_count - len(df)} duplicate records\")\n",
    "        \n",
    "        df['City'] = df['City'].str.strip()\n",
    "        \n",
    "        # Fill missing values in categorical columns\n",
    "        missing_best_time = df['Best Time to visit'].isna().sum()\n",
    "        if missing_best_time > 0:\n",
    "            logger.info(f\"Filling {missing_best_time} missing values in 'Best Time to visit'\")\n",
    "            df['Best Time to visit'] = df['Best Time to visit'].fillna('Throughout the year')\n",
    "        \n",
    "        # Add columns for feature engineering\n",
    "        df['Year_round'] = df['Best Time to visit'].str.contains('Throughout the year').astype(int)\n",
    "        \n",
    "        logger.info(f\"âœ… Dataset Loaded with {len(df)} records.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Error loading data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Cache for weather and traffic data to avoid regenerating during testing\n",
    "_weather_cache = {}\n",
    "_traffic_cache = {}\n",
    "\n",
    "# Enhanced Weather Data Generation with more realistic distributions and caching\n",
    "def get_enhanced_weather_data(cities, use_cache=True):\n",
    "    global _weather_cache\n",
    "    \n",
    "    # Return cached data if available and requested\n",
    "    if use_cache and _weather_cache and set(cities).issubset(set(_weather_cache.keys())):\n",
    "        logger.info(\"Using cached weather data\")\n",
    "        return {city: _weather_cache[city] for city in cities}\n",
    "    \n",
    "    logger.info(\"Generating new weather data\")\n",
    "    weather_data = {}\n",
    "    # More realistic weather conditions with seasonal variation\n",
    "    conditions = {\n",
    "        \"Clear\": {\"quality_range\": (7, 10), \"prob\": 0.4, \"temp_range\": (18, 28)},\n",
    "        \"Cloudy\": {\"quality_range\": (5, 8), \"prob\": 0.3, \"temp_range\": (15, 25)},\n",
    "        \"Rainy\": {\"quality_range\": (3, 6), \"prob\": 0.2, \"temp_range\": (12, 22)},\n",
    "        \"Sunny\": {\"quality_range\": (8, 10), \"prob\": 0.1, \"temp_range\": (22, 35)}\n",
    "    }\n",
    "    \n",
    "    # Use a fixed seed for reproducibility during testing\n",
    "    random.seed(42)\n",
    "    \n",
    "    for city in cities:\n",
    "        # Use weighted random choice for condition\n",
    "        condition = random.choices(\n",
    "            list(conditions.keys()), \n",
    "            weights=[cond['prob'] for cond in conditions.values()], \n",
    "            k=1\n",
    "        )[0]\n",
    "        \n",
    "        # Get quality range for the selected condition\n",
    "        quality_range = conditions[condition][\"quality_range\"]\n",
    "        temp_range = conditions[condition][\"temp_range\"]\n",
    "        \n",
    "        quality = random.randint(quality_range[0], quality_range[1])\n",
    "        \n",
    "        weather_data[city] = {\n",
    "            \"quality\": quality,\n",
    "            \"condition\": condition,\n",
    "            \"temp\": random.randint(temp_range[0], temp_range[1])\n",
    "        }\n",
    "    \n",
    "    # Update cache\n",
    "    _weather_cache.update(weather_data)\n",
    "    \n",
    "    # Reset random seed\n",
    "    random.seed(None)\n",
    "    \n",
    "    return weather_data\n",
    "\n",
    "# Enhanced Traffic Data with seasonal variation and caching\n",
    "def get_enhanced_traffic_data(cities, use_cache=True):\n",
    "    global _traffic_cache\n",
    "    \n",
    "    # Return cached data if available and requested\n",
    "    if use_cache and _traffic_cache and set(cities).issubset(set(_traffic_cache.keys())):\n",
    "        logger.info(\"Using cached traffic data\")\n",
    "        return {city: _traffic_cache[city] for city in cities}\n",
    "    \n",
    "    logger.info(\"Generating new traffic data\")\n",
    "    traffic_data = {}\n",
    "    \n",
    "    # Define different city types with different traffic distributions\n",
    "    city_types = {\n",
    "        \"metropolitan\": {\"traffic_range\": (6, 10), \"prob\": 0.3, \"attractions\": (8, 15)},\n",
    "        \"urban\": {\"traffic_range\": (4, 8), \"prob\": 0.4, \"attractions\": (6, 10)},\n",
    "        \"suburban\": {\"traffic_range\": (2, 6), \"prob\": 0.2, \"attractions\": (4, 8)},\n",
    "        \"rural\": {\"traffic_range\": (1, 4), \"prob\": 0.1, \"attractions\": (2, 5)}\n",
    "    }\n",
    "    \n",
    "    # Use a fixed seed for reproducibility during testing\n",
    "    random.seed(42)\n",
    "    \n",
    "    for city in cities:\n",
    "        # Assign city type based on probability\n",
    "        city_type = random.choices(\n",
    "            list(city_types.keys()), \n",
    "            weights=[type_data['prob'] for type_data in city_types.values()], \n",
    "            k=1\n",
    "        )[0]\n",
    "        \n",
    "        traffic_range = city_types[city_type][\"traffic_range\"]\n",
    "        base_traffic = random.randint(traffic_range[0], traffic_range[1])\n",
    "        \n",
    "        # Number of attractions based on city type\n",
    "        num_attractions = random.randint(*city_types[city_type][\"attractions\"])\n",
    "        \n",
    "        # Create more realistic travel times for attractions based on traffic level\n",
    "        places = {}\n",
    "        for i in range(1, num_attractions + 1):\n",
    "            # More congested cities have longer travel times with higher variance\n",
    "            min_time = 10 + (base_traffic * 2)\n",
    "            max_time = 20 + (base_traffic * 5)\n",
    "            places[f\"{city} Place {i}\"] = random.randint(min_time, max_time)\n",
    "        \n",
    "        traffic_data[city] = {\n",
    "            \"traffic_level\": base_traffic,\n",
    "            \"city_type\": city_type,\n",
    "            \"places\": places,\n",
    "            \"num_attractions\": num_attractions\n",
    "        }\n",
    "    \n",
    "    # Update cache\n",
    "    _traffic_cache.update(traffic_data)\n",
    "    \n",
    "    # Reset random seed\n",
    "    random.seed(None)\n",
    "    \n",
    "    return traffic_data\n",
    "\n",
    "# Feature Engineering Function with more advanced features\n",
    "def engineer_features(df, weather_data, traffic_data):\n",
    "    \"\"\"Create additional features for better predictions\"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info(\"Starting feature engineering...\")\n",
    "    \n",
    "    # Create copies of data to avoid modifying original\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Merge Weather & Traffic Data\n",
    "    for city in df_processed[\"City\"]:\n",
    "        if city in weather_data:\n",
    "            df_processed.loc[df_processed[\"City\"] == city, \"Weather Quality\"] = weather_data[city][\"quality\"]\n",
    "            df_processed.loc[df_processed[\"City\"] == city, \"Temperature\"] = weather_data[city].get(\"temp\", 25)\n",
    "            df_processed.loc[df_processed[\"City\"] == city, \"Weather Condition\"] = weather_data[city].get(\"condition\", \"Unknown\")\n",
    "        if city in traffic_data:\n",
    "            df_processed.loc[df_processed[\"City\"] == city, \"Traffic Level\"] = traffic_data[city][\"traffic_level\"]\n",
    "            df_processed.loc[df_processed[\"City\"] == city, \"City Type\"] = traffic_data[city].get(\"city_type\", \"urban\")\n",
    "            df_processed.loc[df_processed[\"City\"] == city, \"Num Attractions\"] = traffic_data[city].get(\"num_attractions\", 5)\n",
    "\n",
    "    # Fill Missing Values with more intelligent defaults based on similar cities\n",
    "    # For weather quality, group by weather condition\n",
    "    if \"Weather Condition\" in df_processed.columns and \"Weather Quality\" in df_processed.columns:\n",
    "        condition_medians = df_processed.groupby(\"Weather Condition\")[\"Weather Quality\"].median().to_dict()\n",
    "        \n",
    "        # For each missing value, fill with the median of that condition\n",
    "        for condition, median in condition_medians.items():\n",
    "            mask = (df_processed[\"Weather Quality\"].isna()) & (df_processed[\"Weather Condition\"] == condition)\n",
    "            df_processed.loc[mask, \"Weather Quality\"] = median\n",
    "    \n",
    "    # Fill remaining missing values with medians\n",
    "    for col in [\"Weather Quality\", \"Traffic Level\", \"Temperature\", \"Num Attractions\"]:\n",
    "        if col in df_processed.columns:\n",
    "            missing_count = df_processed[col].isna().sum()\n",
    "            if missing_count > 0:\n",
    "                logger.info(f\"Filling {missing_count} missing values in '{col}'\")\n",
    "                df_processed[col] = df_processed[col].fillna(df_processed[col].median())\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    categorical_cols = [\"Weather Condition\", \"City Type\"]\n",
    "    for col in categorical_cols:\n",
    "        if col in df_processed.columns:\n",
    "            df_processed = pd.get_dummies(df_processed, columns=[col], prefix=col.replace(\" \", \"_\"))\n",
    "    \n",
    "    # Create interaction features\n",
    "    df_processed[\"Weather_Traffic_Interaction\"] = df_processed[\"Weather Quality\"] * (10 - df_processed[\"Traffic Level\"]) / 10\n",
    "    \n",
    "    # Temperature comfort score (optimal around 24-26Â°C)\n",
    "    if \"Temperature\" in df_processed.columns:\n",
    "        df_processed[\"Temp_Comfort\"] = 10 - abs(df_processed[\"Temperature\"] - 24) / 2\n",
    "        df_processed[\"Temp_Comfort\"] = df_processed[\"Temp_Comfort\"].clip(0, 10)\n",
    "    \n",
    "    # Attractions density score\n",
    "    if \"Num Attractions\" in df_processed.columns:\n",
    "        max_attractions = df_processed[\"Num Attractions\"].max()\n",
    "        df_processed[\"Attractions_Score\"] = df_processed[\"Num Attractions\"] / max_attractions * 10\n",
    "    \n",
    "    # Create target variable with more nuanced definition\n",
    "    df_processed[\"Destination Score\"] = (\n",
    "        (0.35 * df_processed[\"Weather Quality\"]) + \n",
    "        (0.30 * (10 - df_processed[\"Traffic Level\"])) + \n",
    "        (0.15 * df_processed[\"Year_round\"]) +\n",
    "        (0.10 * df_processed.get(\"Temp_Comfort\", 5)) +\n",
    "        (0.10 * df_processed.get(\"Attractions_Score\", 5))\n",
    "    )\n",
    "    \n",
    "    # Create classification target\n",
    "    df_processed[\"Good Destination\"] = (df_processed[\"Destination Score\"] >= 6.5).astype(int)\n",
    "    \n",
    "    # Log class balance\n",
    "    class_counts = df_processed[\"Good Destination\"].value_counts()\n",
    "    logger.info(f\"Class distribution - Good: {class_counts.get(1, 0)}, Not Good: {class_counts.get(0, 0)}\")\n",
    "    \n",
    "    logger.info(f\"Feature engineering completed in {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Perform feature selection to improve model performance\n",
    "def select_features(df_processed):\n",
    "    \"\"\"Select most important features to reduce dimensionality\"\"\"\n",
    "    # Basic features we always want to include\n",
    "    base_features = [\"Weather Quality\", \"Traffic Level\", \"Year_round\"]\n",
    "    \n",
    "    # Additional features if they exist\n",
    "    additional_features = [\n",
    "        \"Temperature\", \"Weather_Traffic_Interaction\", \n",
    "        \"Temp_Comfort\", \"Attractions_Score\", \"Num Attractions\"\n",
    "    ]\n",
    "    \n",
    "    # One-hot encoded features\n",
    "    weather_condition_features = [col for col in df_processed.columns if col.startswith(\"Weather_Condition_\")]\n",
    "    city_type_features = [col for col in df_processed.columns if col.startswith(\"City_Type_\")]\n",
    "    \n",
    "    # Combine all potential features\n",
    "    potential_features = (\n",
    "        base_features + \n",
    "        [f for f in additional_features if f in df_processed.columns] +\n",
    "        weather_condition_features +\n",
    "        city_type_features\n",
    "    )\n",
    "    \n",
    "    # Check for high correlation between features\n",
    "    if len(potential_features) > 1:\n",
    "        corr_matrix = df_processed[potential_features].corr().abs()\n",
    "        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        \n",
    "        # Find features with correlation > 0.85\n",
    "        to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.85)]\n",
    "        \n",
    "        if to_drop:\n",
    "            logger.info(f\"Dropping highly correlated features: {to_drop}\")\n",
    "            potential_features = [f for f in potential_features if f not in to_drop]\n",
    "    \n",
    "    logger.info(f\"Selected {len(potential_features)} features for model training\")\n",
    "    return potential_features\n",
    "\n",
    "# Train Multiple Models with cross-validation and Select Best\n",
    "def train_recommendation_models(df_processed, features):\n",
    "    start_time = time.time()\n",
    "    logger.info(f\"ðŸš€ Training models using {len(features)} features\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df_processed[features].values\n",
    "    y = df_processed[\"Good Destination\"].values\n",
    "    \n",
    "    # Check for class imbalance\n",
    "    class_counts = np.bincount(y)\n",
    "    if len(class_counts) > 1 and min(class_counts) / sum(class_counts) < 0.2:\n",
    "        logger.warning(f\"Class imbalance detected: {class_counts}\")\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Split data with stratification to handle imbalanced classes\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Model dictionary with optimized parameters\n",
    "    models = {\n",
    "        \"Random Forest\": RandomForestClassifier(\n",
    "            random_state=42,\n",
    "            n_jobs=-1,  # Use all available cores\n",
    "            class_weight='balanced' if len(class_counts) > 1 and min(class_counts) / sum(class_counts) < 0.3 else None\n",
    "        ),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(\n",
    "            random_state=42,\n",
    "            validation_fraction=0.1,\n",
    "            n_iter_no_change=5,\n",
    "            tol=1e-4\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Parameter grid for GridSearchCV\n",
    "    param_grids = {\n",
    "        \"Random Forest\": {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 2],\n",
    "            'max_features': ['sqrt', 'log2']\n",
    "        },\n",
    "        \"Gradient Boosting\": {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.05, 0.1],\n",
    "            'max_depth': [3, 5],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'max_features': ['sqrt', 'log2']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    results = {}\n",
    "    \n",
    "    # For small datasets, adjust the param grids\n",
    "    if len(X_train) < 200:\n",
    "        logger.info(\"Small dataset detected, simplifying parameter grid\")\n",
    "        for model_name in param_grids:\n",
    "            for param in param_grids[model_name]:\n",
    "                param_grids[model_name][param] = param_grids[model_name][param][:1] # Just use first value\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for name, model in models.items():\n",
    "        model_start_time = time.time()\n",
    "        logger.info(f\"Training {name}...\")\n",
    "        \n",
    "        # Use GridSearchCV for hyperparameter tuning\n",
    "        grid_search = GridSearchCV(\n",
    "            model, param_grids[name], cv=min(5, len(X_train) // 10) if len(X_train) < 50 else 5, \n",
    "            scoring='f1_weighted', n_jobs=-1 if len(X_train) > 1000 else 1\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model_params = grid_search.best_estimator_\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            y_pred = best_model_params.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "            \n",
    "            # Generate detailed classification report\n",
    "            report = classification_report(y_test, y_pred, target_names=['Not Good', 'Good'], output_dict=True)\n",
    "            \n",
    "            # Get feature importance\n",
    "            if hasattr(best_model_params, 'feature_importances_'):\n",
    "                feature_importance = best_model_params.feature_importances_\n",
    "            else:\n",
    "                feature_importance = np.zeros(len(features))\n",
    "            \n",
    "            results[name] = {\n",
    "                'model': best_model_params,\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'feature_importance': feature_importance,\n",
    "                'best_params': grid_search.best_params_,\n",
    "                'classification_report': report\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"  {name} - Accuracy: {accuracy:.4f}, F1: {f1:.4f}, \"\n",
    "                      f\"Time: {time.time() - model_start_time:.2f}s\")\n",
    "            \n",
    "            # Track best model\n",
    "            if f1 > best_score:\n",
    "                best_score = f1\n",
    "                best_model = best_model_params\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error training {name}: {str(e)}\")\n",
    "    \n",
    "    if not best_model:\n",
    "        logger.error(\"Failed to train any models successfully\")\n",
    "        return None, None, features, scaler\n",
    "    \n",
    "    # Print feature importance for the best model\n",
    "    logger.info(\"\\nðŸ” Feature Importance for Best Model:\")\n",
    "    best_model_name = max(results, key=lambda k: results[k]['f1'])\n",
    "    importance = results[best_model_name]['feature_importance']\n",
    "    \n",
    "    feature_importance_dict = {}\n",
    "    for i, feat in enumerate(features):\n",
    "        logger.info(f\"  {feat}: {importance[i]:.4f}\")\n",
    "        feature_importance_dict[feat] = importance[i]\n",
    "    \n",
    "    # Sort features by importance\n",
    "    sorted_features = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    logger.info(f\"Most important features: {sorted_features[:3]}\")\n",
    "    \n",
    "    logger.info(f\"Model training completed in {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    return best_model, results, features, scaler\n",
    "\n",
    "# Save Model with all necessary components\n",
    "def save_enhanced_model(model, features, scaler, results=None, filename=\"travel_recommendation_enhanced.pkl\"):\n",
    "    if model is None:\n",
    "        logger.error(\"No model to save\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        model_data = {\n",
    "            \"model\": model,\n",
    "            \"features\": features,\n",
    "            \"scaler\": scaler,\n",
    "            \"version\": \"2.0\",\n",
    "            \"training_date\": datetime.now(),\n",
    "            \"results\": results\n",
    "        }\n",
    "        \n",
    "        with open(filename, \"wb\") as file:\n",
    "            pickle.dump(model_data, file)\n",
    "        \n",
    "        logger.info(f\"âœ… Enhanced Model Saved as {filename}\")\n",
    "        \n",
    "        # Also save as joblib file for faster loading\n",
    "        joblib_filename = filename.replace('.pkl', '.joblib')\n",
    "        joblib.dump(model_data, joblib_filename)\n",
    "        logger.info(f\"âœ… Enhanced Model also saved as {joblib_filename}\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving model: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Load a saved model\n",
    "def load_model(filename=\"travel_recommendation_enhanced.joblib\"):\n",
    "    try:\n",
    "        # Try joblib first (faster loading)\n",
    "        if os.path.exists(filename):\n",
    "            model_data = joblib.load(filename)\n",
    "            logger.info(f\"âœ… Model loaded successfully from {filename}\")\n",
    "        # Fall back to pickle if joblib file not found\n",
    "        elif os.path.exists(filename.replace('.joblib', '.pkl')):\n",
    "            with open(filename.replace('.joblib', '.pkl'), \"rb\") as file:\n",
    "                model_data = pickle.load(file)\n",
    "            logger.info(f\"âœ… Model loaded successfully from {filename.replace('.joblib', '.pkl')}\")\n",
    "        else:\n",
    "            logger.error(f\"Model file not found: {filename}\")\n",
    "            return None\n",
    "            \n",
    "        # Check that model_data has expected keys\n",
    "        required_keys = [\"model\", \"features\", \"scaler\"]\n",
    "        missing_keys = [key for key in required_keys if key not in model_data]\n",
    "        \n",
    "        if missing_keys:\n",
    "            logger.error(f\"Model data missing required keys: {missing_keys}\")\n",
    "            return None\n",
    "            \n",
    "        return model_data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Generate Recommendations using the trained model\n",
    "def generate_smart_recommendations(df, model, weather_data, traffic_data, features, scaler, num_recommendations=5):\n",
    "    if model is None:\n",
    "        logger.error(\"No model available for generating recommendations\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(\"\\nðŸŽ¯ Generating Smart Recommendations...\")\n",
    "    \n",
    "    try:\n",
    "        # Engineer features for all cities\n",
    "        df_processed = engineer_features(df, weather_data, traffic_data)\n",
    "        \n",
    "        # Check if all required features exist\n",
    "        missing_features = [f for f in features if f not in df_processed.columns]\n",
    "        if missing_features:\n",
    "            logger.warning(f\"Missing features in processed data: {missing_features}\")\n",
    "            # Create missing features with default values\n",
    "            for feature in missing_features:\n",
    "                df_processed[feature] = 0\n",
    "        \n",
    "        # Prepare feature matrix\n",
    "        X = df_processed[features].values\n",
    "        X_scaled = scaler.transform(X)\n",
    "        \n",
    "        # Get model predictions\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            # If model supports probability, use it for ranking\n",
    "            proba = model.predict_proba(X_scaled)\n",
    "            df_processed[\"Recommendation Score\"] = proba[:, 1]  # Probability of being a good destination\n",
    "        else:\n",
    "            # Fall back to decision function or manual scoring\n",
    "            df_processed[\"Recommendation Score\"] = model.predict(X_scaled).astype(float)\n",
    "        \n",
    "        # Calculate a weighted final score with confidence adjustment\n",
    "        df_processed[\"Final Score\"] = (\n",
    "            (0.40 * df_processed[\"Weather Quality\"] / 10) +  # Normalize to 0-1\n",
    "            (0.25 * (10 - df_processed[\"Traffic Level\"]) / 10) +  # Inverse and normalize\n",
    "            (0.20 * df_processed[\"Recommendation Score\"]) +  # Already 0-1\n",
    "            (0.10 * df_processed[\"Year_round\"]) +  # 0 or 1\n",
    "            (0.05 * df_processed.get(\"Temp_Comfort\", 5) / 10)  # Normalize to 0-1\n",
    "        ) * 10  # Scale to 0-10\n",
    "        \n",
    "        # Get top recommendations\n",
    "        recommendations = df_processed.sort_values(by=\"Final Score\", ascending=False).head(num_recommendations)\n",
    "        \n",
    "        # Create a more detailed recommendation output\n",
    "        columns_to_include = [\"City\", \"Weather Quality\", \"Traffic Level\", \n",
    "                             \"Recommendation Score\", \"Final Score\", \"Year_round\"]\n",
    "        \n",
    "        optional_columns = [\"Temp_Comfort\", \"Attractions_Score\", \"Num Attractions\", \"Temperature\"]\n",
    "        for col in optional_columns:\n",
    "            if col in df_processed.columns:\n",
    "                columns_to_include.append(col)\n",
    "        \n",
    "        detailed_recommendations = recommendations[columns_to_include]\n",
    "        \n",
    "        # Round scores for cleaner display\n",
    "        numeric_columns = detailed_recommendations.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_columns:\n",
    "            detailed_recommendations[col] = detailed_recommendations[col].round(2)\n",
    "        \n",
    "        return detailed_recommendations\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating recommendations: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# NEW: Function to get similar cities based on a reference city\n",
    "def get_similar_cities(reference_city, df, weather_data, traffic_data, features, scaler, top_n=5):\n",
    "    try:\n",
    "        if reference_city not in df[\"City\"].values:\n",
    "            logger.error(f\"Reference city '{reference_city}' not found in dataset\")\n",
    "            return None\n",
    "        \n",
    "        # Engineer features\n",
    "        df_processed = engineer_features(df, weather_data, traffic_data)\n",
    "        \n",
    "        # Fill missing features\n",
    "        missing_features = [f for f in features if f not in df_processed.columns]\n",
    "        for feature in missing_features:\n",
    "            df_processed[feature] = 0\n",
    "        \n",
    "        # Get feature values for reference city\n",
    "        reference_vector = df_processed.loc[df_processed[\"City\"] == reference_city, features].values[0]\n",
    "        reference_scaled = scaler.transform([reference_vector])[0]\n",
    "        \n",
    "        # Calculate distances for all cities\n",
    "        distances = []\n",
    "        for idx, row in df_processed.iterrows():\n",
    "            if row[\"City\"] != reference_city:  # Skip the reference city itself\n",
    "                city_vector = row[features].values\n",
    "                city_scaled = scaler.transform([city_vector])[0]\n",
    "                \n",
    "                # Calculate Euclidean distance\n",
    "                distance = np.sqrt(np.sum((reference_scaled - city_scaled)**2))\n",
    "                distances.append((row[\"City\"], distance))\n",
    "        \n",
    "        # Sort by distance and get top N\n",
    "        similar_cities = sorted(distances, key=lambda x: x[1])[:top_n]\n",
    "        \n",
    "        # Create dataframe of similar cities with details\n",
    "        similar_df = pd.DataFrame([\n",
    "            {\"City\": city, \"Similarity Score\": round(10 - dist, 2)} \n",
    "            for city, dist in similar_cities\n",
    "        ])\n",
    "        \n",
    "        return similar_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error finding similar cities: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# NEW: User preferences based recommendation\n",
    "def recommend_based_on_preferences(df, model_data, user_preferences, num_recommendations=5):\n",
    "    \"\"\"\n",
    "    Generate recommendations based on user preferences\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Original city data\n",
    "    model_data (dict): Loaded model data containing model, features, and scaler\n",
    "    user_preferences (dict): Dictionary with user preferences like:\n",
    "        - weather_importance (float): 0-1 importance of good weather\n",
    "        - crowd_importance (float): 0-1 importance of less crowds\n",
    "        - attractions_importance (float): 0-1 importance of attractions\n",
    "        - season (str): Preferred season ('Summer', 'Winter', 'Spring', 'Fall')\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Top recommendations based on user preferences\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = model_data[\"model\"]\n",
    "        features = model_data[\"features\"]\n",
    "        scaler = model_data[\"scaler\"]\n",
    "        \n",
    "        # Get base data\n",
    "        cities = df[\"City\"].tolist()\n",
    "        weather_data = get_enhanced_weather_data(cities)\n",
    "        traffic_data = get_enhanced_traffic_data(cities)\n",
    "        \n",
    "        # Process data\n",
    "        df_processed = engineer_features(df, weather_data, traffic_data)\n",
    "        \n",
    "        # Check if all required features exist\n",
    "        missing_features = [f for f in features if f not in df_processed.columns]\n",
    "        for feature in missing_features:\n",
    "            df_processed[feature] = 0\n",
    "        \n",
    "        # Prepare feature matrix for model prediction\n",
    "        X = df_processed[features].values\n",
    "        X_scaled = scaler.transform(X)\n",
    "        \n",
    "        # Get model predictions\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            proba = model.predict_proba(X_scaled)\n",
    "            df_processed[\"Recommendation Score\"] = proba[:, 1]\n",
    "        else:\n",
    "            df_processed[\"Recommendation Score\"] = model.predict(X_scaled).astype(float)\n",
    "        \n",
    "        # Apply user preferences to adjust scores\n",
    "        # Normalize importance values to ensure they sum to 1\n",
    "        weather_importance = user_preferences.get(\"weather_importance\", 0.3)\n",
    "        crowd_importance = user_preferences.get(\"crowd_importance\", 0.3)\n",
    "        attractions_importance = user_preferences.get(\"attractions_importance\", 0.2)\n",
    "        year_round_importance = user_preferences.get(\"year_round_importance\", 0.1)\n",
    "        temp_importance = user_preferences.get(\"temperature_importance\", 0.1)\n",
    "        \n",
    "        # Adjust for season preference if specified\n",
    "        season = user_preferences.get(\"season\", None)\n",
    "        if season:\n",
    "            # Create a season score based on best time to visit\n",
    "            df_processed[\"Season Match\"] = df[\"Best Time to visit\"].str.contains(season, case=False).astype(float)\n",
    "            season_importance = 0.2  # Give season preference significant weight\n",
    "            \n",
    "            # Normalize other weights\n",
    "            total = weather_importance + crowd_importance + attractions_importance + year_round_importance + temp_importance\n",
    "            factor = (1 - season_importance) / total\n",
    "            \n",
    "            weather_importance *= factor\n",
    "            crowd_importance *= factor\n",
    "            attractions_importance *= factor\n",
    "            year_round_importance *= factor\n",
    "            temp_importance *= factor\n",
    "            \n",
    "            # Include season in final score calculation\n",
    "            df_processed[\"Final Score\"] = (\n",
    "                (weather_importance * df_processed[\"Weather Quality\"] / 10) +\n",
    "                (crowd_importance * (10 - df_processed[\"Traffic Level\"]) / 10) +\n",
    "                (attractions_importance * df_processed.get(\"Attractions_Score\", 5) / 10) +\n",
    "                (year_round_importance * df_processed[\"Year_round\"]) +\n",
    "                (temp_importance * df_processed.get(\"Temp_Comfort\", 5) / 10) +\n",
    "                (season_importance * df_processed[\"Season Match\"])\n",
    "            ) * 10\n",
    "        else:\n",
    "            # No season preference, use standard weights\n",
    "            df_processed[\"Final Score\"] = (\n",
    "                (weather_importance * df_processed[\"Weather Quality\"] / 10) +\n",
    "                (crowd_importance * (10 - df_processed[\"Traffic Level\"]) / 10) +\n",
    "                (attractions_importance * df_processed.get(\"Attractions_Score\", 5) / 10) +\n",
    "                (year_round_importance * df_processed[\"Year_round\"]) +\n",
    "                (temp_importance * df_processed.get(\"Temp_Comfort\", 5) / 10)\n",
    "            ) * 10\n",
    "        \n",
    "        # Get top recommendations\n",
    "        recommendations = df_processed.sort_values(by=\"Final Score\", ascending=False).head(num_recommendations)\n",
    "        \n",
    "        # Create a more detailed recommendation output\n",
    "        columns_to_include = [\"City\", \"Weather Quality\", \"Traffic Level\", \n",
    "                             \"Recommendation Score\", \"Final Score\", \"Year_round\"]\n",
    "        \n",
    "        optional_columns = [\"Temp_Comfort\", \"Attractions_Score\", \"Num Attractions\", \"Temperature\"]\n",
    "        for col in optional_columns:\n",
    "            if col in df_processed.columns:\n",
    "                columns_to_include.append(col)\n",
    "        \n",
    "        detailed_recommendations = recommendations[columns_to_include]\n",
    "        \n",
    "        # Round scores for cleaner display\n",
    "        numeric_columns = detailed_recommendations.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_columns:\n",
    "            detailed_recommendations[col] = detailed_recommendations[col].round(2)\n",
    "        \n",
    "        return detailed_recommendations\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating recommendations: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Main Execution Pipeline with enhanced process and error handling\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    logger.info(\"Starting Travel Recommendation System\")\n",
    "    \n",
    "    df = load_data()\n",
    "    if df is None:\n",
    "        logger.error(\"Failed to load data. Exiting.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        cities = df[\"City\"].tolist()\n",
    "        logger.info(f\"Processing {len(cities)} cities\")\n",
    "        \n",
    "        # Generate enhanced data\n",
    "        weather_data = get_enhanced_weather_data(cities)\n",
    "        traffic_data = get_enhanced_traffic_data(cities)\n",
    "        \n",
    "        # Engineer features\n",
    "        df_processed = engineer_features(df, weather_data, traffic_data)\n",
    "        \n",
    "        # Select most important features\n",
    "        selected_features = select_features(df_processed)\n",
    "        \n",
    "        # Train and select best model\n",
    "        best_model, model_results, features, scaler = train_recommendation_models(df_processed, selected_features)\n",
    "        \n",
    "        if best_model is None:\n",
    "            logger.error(\"Model training failed. Exiting.\")\n",
    "            return False\n",
    "        \n",
    "        # Save the enhanced model with all components\n",
    "        save_success = save_enhanced_model(best_model, features, scaler, model_results)\n",
    "        \n",
    "        if not save_success:\n",
    "            logger.warning(\"Failed to save model\")\n",
    "        \n",
    "        # Generate recommendations\n",
    "        recommendations = generate_smart_recommendations(\n",
    "            df, best_model, weather_data, traffic_data, features, scaler\n",
    "        )\n",
    "        \n",
    "        if recommendations is None:\n",
    "            logger.error(\"Failed to generate recommendations\")\n",
    "            return False\n",
    "        \n",
    "        logger.info(\"\\nðŸ™ï¸ Top 5 Recommended Destinations:\\n\")\n",
    "        logger.info(recommendations)\n",
    "        \n",
    "        # Visualize the top destinations\n",
    "        logger.info(\"\\nðŸ“Š Recommendation Analysis:\")\n",
    "        logger.info(f\"  Best destination: {recommendations.iloc[0]['City']} with score {recommendations.iloc[0]['Final Score']:.2f}/10\")\n",
    "        \n",
    "        weather_friendly = df_processed.sort_values('Weather Quality', ascending=False)['City'].values[0]\n",
    "        logger.info(f\"  Most weather-friendly: {weather_friendly}\")\n",
    "        \n",
    "        least_crowded = df_processed.sort_values('Traffic Level')['City'].values[0]\n",
    "        logger.info(f\"  Least crowded: {least_crowded}\")\n",
    "        \n",
    "        logger.info(f\"Total execution time: {time.time() - start_time:.2f} seconds\")\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
